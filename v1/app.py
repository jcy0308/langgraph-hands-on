import streamlit as st
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA
import pickle
import os
from dotenv import load_dotenv

load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("api keyë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”.")

# FAISS ë²¡í„° DB ë¡œë“œ
FAISS_INDEX_PATH = "faiss_index.pkl"

st.title("LangChain + FAISS ê¸°ë°˜ RAG ê²€ìƒ‰ ì‹œìŠ¤í…œ")
st.subheader("ğŸ’¡ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´, ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")

# FAISS ë²¡í„° DB ë¡œë“œ
if os.path.exists(FAISS_INDEX_PATH):
    with open(FAISS_INDEX_PATH, "rb") as f:
        vectorstore = pickle.load(f)
    retriever = vectorstore.as_retriever()
else:
    st.error("âŒ FAISS ë²¡í„° DBê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € `data_ingestion.py`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    st.stop()

# LangChain RAG QA Chain ì„¤ì •
llm = OpenAI(model_name="gpt-4")
qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

# ì§ˆë¬¸ ì…ë ¥
query = st.text_input("ğŸ” ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
if query:
    with st.spinner("ê²€ìƒ‰ ì¤‘..."):
        response = qa_chain.run(query)
        st.write("### ğŸ”¹ AI ì‘ë‹µ:")
        st.write(response)
