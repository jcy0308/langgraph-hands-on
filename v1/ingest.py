import os
import pickle
import pdfplumber
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.vectorstores import FAISS
from langchain.schema import Document
import ssl
import certifi

ssl._create_default_https_context = ssl._create_unverified_context

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# PDF íŒŒì¼ ê²½ë¡œ (ìƒìœ„ í´ë”ì— ìœ„ì¹˜í•œ íŒŒì¼)
PDF_FILE_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "sample.pdf"))

# PDF ë¬¸ì„œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
if not os.path.exists(PDF_FILE_PATH):
    raise FileNotFoundError(f"âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {PDF_FILE_PATH}")

print(f"ğŸ“„ PDF ë¬¸ì„œ {PDF_FILE_PATH} ë¡œë“œ ì¤‘...")

# PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
documents = []
with pdfplumber.open(PDF_FILE_PATH) as pdf:
    for page in pdf.pages:
        text = page.extract_text()
        if text:
            documents.append(Document(page_content=text))

# í…ìŠ¤íŠ¸ ë¶„í• 
print("ğŸ”„ ë¬¸ì„œ ë¶„í•  ì¤‘...")
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
texts = text_splitter.split_documents(documents)

# Hugging Face ì„ë² ë”© ìƒì„± (ë¬´ë£Œ ëŒ€ì²´)
print("ğŸ” ë¬¸ì„œë¥¼ ë²¡í„°í™” ì¤‘...")
embeddings = SentenceTransformerEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# FAISS ì¸ë±ìŠ¤ ìƒì„±
vectorstore = FAISS.from_documents(texts, embeddings)

# FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œì»¬ ì €ì¥
FAISS_INDEX_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "faiss_index.pkl"))
with open(FAISS_INDEX_PATH, "wb") as f:
    pickle.dump(vectorstore, f)

print(f"âœ… {len(texts)} ê°œì˜ ë¬¸ì„œë¥¼ FAISSì— ì €ì¥ ì™„ë£Œ! (ê²½ë¡œ: {FAISS_INDEX_PATH})")
